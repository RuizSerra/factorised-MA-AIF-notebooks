{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree approach\n",
    "\n",
    "Each policy is constructed from a tree, given the action space (e.g. $\\{\\texttt{c}, \\texttt{d}\\}$)\n",
    "and the policy length (i.e., the depth of the tree). The tree is constructed recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from games import *\n",
    "\n",
    "## SCAFFOLDING\n",
    "num_actions = 2\n",
    "num_agents = 3\n",
    "\n",
    "A_params = torch.ones((num_agents, num_actions, num_actions))\n",
    "A = A_params / A_params.sum(dim=1, keepdim=True)\n",
    "\n",
    "B_params = torch.stack([\n",
    "                torch.eye(num_actions) \n",
    "                for _ in range(num_actions) \n",
    "                for _ in range(num_agents)\n",
    "            ]).reshape(num_agents, num_actions, num_actions, num_actions)\n",
    "B = B_params / B_params.sum(dim=2, keepdim=True)\n",
    "\n",
    "log_C = prisoners_dilemma_2player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_efe(u, q_s_u, A, log_C):\n",
    "    '''\n",
    "    Compute the Expected Free Energy (EFE) of a given action\n",
    "    \n",
    "    Args:\n",
    "        u (torch.Tensor): action\n",
    "        q_s_u (torch.Tensor): variational posterior over states given action\n",
    "        A (torch.Tensor): observation likelihood model\n",
    "        log_C (torch.Tensor): log preference over observations\n",
    "\n",
    "    Returns:\n",
    "        EFE (torch.Tensor): Expected Free Energy for a given action\n",
    "    '''\n",
    "    EFE = 0 \n",
    "    ambiguity = 0 \n",
    "    risk = 0 \n",
    "    salience = 0 \n",
    "    pragmatic_value = 0 \n",
    "    novelty = 0 \n",
    "    \n",
    "    # Predictive observation posterior -------------------------------------\n",
    "    # (per factor 'f' and per possible action 'u')\n",
    "    # E_{q(s'|u)}[p(o | s)]\n",
    "    q_o_u = torch.einsum(\n",
    "        'fos,fs->fo',\n",
    "        A,         # (f, o, s)\n",
    "        q_s_u      # (f, s)\n",
    "    )              # (f, o)\n",
    "    \n",
    "    # If ego was to take action u, the observation o_i would be guaranteed\n",
    "    # to be o_i = u, so replace q(o_i | u) = one_hot(u) for this action\n",
    "    q_o_u[0] = F.one_hot(u, num_actions).to(torch.float)\n",
    "\n",
    "    # EFE computation -------------------------------------------------------\n",
    "\n",
    "    # Per-factor terms\n",
    "    for factor_idx in range(num_agents):\n",
    "\n",
    "        # Expected ambiguity term (per factor) -------------------------\n",
    "        H = -torch.diag(A[factor_idx] @ torch.log(A[factor_idx] + EPSILON))  # Conditional (pseudo?) entropy (of the generated emissions matrix)\n",
    "        assert H.ndimension() == 1, \"H is not a 1-dimensional tensor\"\n",
    "\n",
    "        s_pred = q_s_u[factor_idx]  # shape (2, )\n",
    "        assert s_pred.ndimension() == 1, \"s_pred is not a 1-dimensional tensor\"\n",
    "        \n",
    "        ambiguity += (H @ s_pred) # Ambiguity is conditional entropy of emissions\n",
    "        # FIXME: not sure if these definitions are correct\n",
    "        # risk[u_i] += (o_pred @ (torch.log(o_pred + EPSILON)))  - (o_pred @ log_C_modality) # Risk is negative posterior predictive entropy minus pragmatic value\n",
    "        # salience[u_i] += -(o_pred @ (torch.log(o_pred + EPSILON)))  - (H @ s_pred) # Salience is negative posterior predictive entropy minus ambiguity (0)\n",
    "        # pragmatic_value[u_i] += (o_pred @ log_C_modality) # Pragmatic value is negative cross-entropy\n",
    "\n",
    "    # Joint predictive observation posterior ---------------------------\n",
    "    # q(o_i, o_j, o_k | u)\n",
    "    # Create the einsum subscripts string dynamically for n_agents\n",
    "    # e.g., if n_agents = 3, this will be 'i,j,k->ijk'\n",
    "    einsum_str = (\n",
    "        ','.join([chr(105 + i) for i in range(num_agents)]) \n",
    "        + '->' \n",
    "        + ''.join([chr(105 + i) for i in range(num_agents)])\n",
    "    )\n",
    "    q_o_joint_u = torch.einsum(\n",
    "        einsum_str, \n",
    "        *[q_o_u[i] for i in range(num_agents)]\n",
    "    )\n",
    "    # assert q_o_joint_u.shape == (num_actions, ) * (num_agents), (\n",
    "    #     f\"q_o_joint_u shape {q_o_joint_u.shape} != {(num_actions, ) * (num_agents)}\"\n",
    "    # )\n",
    "    # assert torch.allclose(q_o_joint_u.sum(), torch.tensor(1.0)), (\n",
    "    #     f\"q_o_joint_u sum {q_o_joint_u.sum()} != 1.0\"\n",
    "    # )\n",
    "\n",
    "    # Risk term (joint) ------------------------------------------------\n",
    "    # i.e. KL[q(o|u) || p*(o)]\n",
    "    risk = torch.tensordot(\n",
    "        (torch.log(q_o_joint_u + EPSILON) - log_C),\n",
    "        q_o_joint_u,\n",
    "        dims=num_agents\n",
    "    )\n",
    "\n",
    "    # Novelty ----------------------------------------------------------\n",
    "    # if self.compute_novelty:\n",
    "    #     novelty[u] += self.compute_A_novelty(u)  # TODO: some sort of regularisation, novelty can be really large\n",
    "    #     # TODO: B novelty?\n",
    "\n",
    "    EFE = ambiguity + risk - novelty\n",
    "    # assert not torch.any(torch.isnan(EFE)), f\"EFE has NaN: {EFE}\"\n",
    "    # assert torch.allclose(risk[u_i] + ambiguity[u_i], EFE[u_i], atol=1e-4), f\"[u_i = {u_i}] risk + ambiguity ({risk[u_i]} + {ambiguity[u_i]}={risk[u_i] + ambiguity[u_i]}) does not equal EFE (={EFE[u_i]})\"\n",
    "    # assert torch.allclose(-salience[u_i] - pragmatic_value[u_i], EFE[u_i], atol=1e-4), f\"[u_i = {u_i}] -salience - pragmatic value (-{salience[u_i]} - {pragmatic_value[u_i]}={-salience[u_i] - pragmatic_value[u_i]}) does not equal EFE (={EFE[u_i]})\"\n",
    "    \n",
    "    return EFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, action=None, depth=0):\n",
    "        self.u = action  # Tensor or action at this node\n",
    "        self.EFE_u = torch.tensor(0)\n",
    "        self.children = []  # List to hold child nodes\n",
    "        self.depth = depth  # Depth of the node\n",
    "\n",
    "    def add_child(self, action):\n",
    "        # Add a child node with the given action (as a tensor)\n",
    "        child = TreeNode(action, self.depth + 1)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"TreeNode(action={self.u}, depth={self.depth}, children={len(self.children)})\"\n",
    "    \n",
    "\n",
    "def build_policy_tree(action_space, max_depth, node=None):\n",
    "    '''Recursive function to build the tree'''\n",
    "\n",
    "    if node is None:\n",
    "        node = TreeNode()\n",
    "\n",
    "    # Base case\n",
    "    if node.depth == max_depth:\n",
    "        return node\n",
    "\n",
    "    # Add a child for each action in the action space \n",
    "    # (recursively until the max depth is reached)\n",
    "    for action in action_space:\n",
    "        child = node.add_child(torch.tensor([action]))\n",
    "        build_policy_tree(action_space, max_depth, node=child)\n",
    "\n",
    "    return node\n",
    "\n",
    "def collect_policies(\n",
    "        node, \n",
    "        q_s,\n",
    "        policy_EFEs=None,\n",
    "        current_policy=None,\n",
    "        ):\n",
    "    '''Function to traverse the tree and collect policies (as tensors)'''\n",
    "    \n",
    "    # Root node case\n",
    "    if current_policy is None:\n",
    "        current_policy = []\n",
    "    if policy_EFEs is None:\n",
    "        policy_EFEs = []\n",
    "    if node.u is None:\n",
    "        node.q_s_u = q_s\n",
    "        new_policy_EFEs = policy_EFEs\n",
    "    # Other nodes\n",
    "    else:\n",
    "        # Compute q(s|u) and EFE(u) for the current node\n",
    "        node.q_s_u = torch.einsum(\n",
    "                'funk,fk->fun',\n",
    "                B,            # (f, u, n, k): factor, u (action), next (state), kurrent (state)\n",
    "                q_s           # (f, k)\n",
    "            )[:, node.u].squeeze()  # (f, u, n) -> (f, n)\n",
    "    \n",
    "        node.EFE_u = compute_efe(node.u, node.q_s_u, A, log_C).unsqueeze(0)\n",
    "        new_policy_EFEs = policy_EFEs + [node.EFE_u]  # EFEs collected top-down\n",
    "    \n",
    "    # Base case (leaf node)\n",
    "    if not node.children:\n",
    "        return [torch.cat(new_policy_EFEs)], [torch.cat(current_policy)]\n",
    "\n",
    "    # Recursive case\n",
    "    EFEs = []\n",
    "    policies = []\n",
    "    for child in node.children:\n",
    "        new_policy = current_policy + [child.u]  # Policies collected bottom-up\n",
    "        node_EFE, sub_policy = collect_policies(child, node.q_s_u, new_policy_EFEs, new_policy)\n",
    "        EFEs.extend(node_EFE)\n",
    "        policies.extend(sub_policy)\n",
    "\n",
    "    return torch.vstack(EFEs), torch.vstack(policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example\n",
    "\n",
    "Below we show how the algorithm works with different policy lengths. Since the `A` and `B` models are uniform/identity, the output is not very exciting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy length: 1\n",
      "EFE[0] = -1.807\n",
      "EFE[1] = -1.807\n",
      "\n",
      "Policy length: 2\n",
      "EFE[0, 0] = -3.614\n",
      "EFE[0, 1] = -3.614\n",
      "EFE[1, 0] = -3.614\n",
      "EFE[1, 1] = -3.614\n",
      "\n",
      "Policy length: 3\n",
      "EFE[0, 0, 0] = -5.421\n",
      "EFE[0, 0, 1] = -5.421\n",
      "EFE[0, 1, 0] = -5.421\n",
      "EFE[0, 1, 1] = -5.421\n",
      "EFE[1, 0, 0] = -5.421\n",
      "EFE[1, 0, 1] = -5.421\n",
      "EFE[1, 1, 0] = -5.421\n",
      "EFE[1, 1, 1] = -5.421\n"
     ]
    }
   ],
   "source": [
    "q_s = torch.tensor([  # Dummy initial beliefs\n",
    "    [0.3, 0.7],\n",
    "    [0.8, 0.2],\n",
    "    [0.3, 0.7],\n",
    "])\n",
    "\n",
    "# Showcase the algorithm for different policy lengths\n",
    "for policy_length in [1, 2, 3]:\n",
    "    print('\\nPolicy length:', policy_length)\n",
    "\n",
    "    # Build the tree with tensors\n",
    "    root = build_policy_tree(torch.arange(num_actions), policy_length)\n",
    "\n",
    "    # Collect policies\n",
    "    EFEs, policies = collect_policies(\n",
    "        root,\n",
    "        q_s=q_s,\n",
    "    )\n",
    "\n",
    "    for p, EFE in zip(policies, EFEs.sum(dim=1)):\n",
    "        print(f'EFE{p.tolist()} = {EFE.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma-aif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
